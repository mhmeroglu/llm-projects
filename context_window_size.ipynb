{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSTYsfLcyOx0"
      },
      "source": [
        "**IMPORT LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LaxDhLqPiOm"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyEkv9dDyqqY"
      },
      "source": [
        "**Google API Key Configuration and Model Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81qVFh9lyoeS"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-dq5C7PzMps"
      },
      "source": [
        "**Context Window and API Retry Configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuZ08YYGcqQa"
      },
      "outputs": [],
      "source": [
        "CONTEXT_WINDOW_MAX = 1000000  # Maximum context capacity in tokens\n",
        "TOKEN_SAFETY_MARGIN = CONTEXT_WINDOW_MAX * 0.8  # Warning threshold at 80% capacity\n",
        "RETRY_ATTEMP_MAX = 3  # Maximum retries for API calls\n",
        "BASE_RETRY_DELAY = 2  # Initial delay between retries in seconds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FSsJUMGzfsp"
      },
      "source": [
        "**Conversation History and Token Tracking**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YwEWvwp0ze1R"
      },
      "outputs": [],
      "source": [
        "conversation_log = []  # Stores conversation history\n",
        "cumulative_tokens = 0  # Tracks total tokens consumed\n",
        "\n",
        "def tokenCount(input_text):\n",
        "    \"\"\"Calculates token count for input text using model's tokenizer.\"\"\"\n",
        "    try:\n",
        "        return model.count_tokens(input_text).total_tokens\n",
        "    except Exception as error:\n",
        "        print(f\"Token counting error: {error}\")\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odJySajazxfP"
      },
      "source": [
        "**Text Generation for Retry Mechanism and Token Tracking**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eySnc-HNzphe"
      },
      "outputs": [],
      "source": [
        "def generate_text(dialog_history):\n",
        "    \"\"\"Generates text response using API with retry mechanism and token tracking.\"\"\"\n",
        "    global cumulative_tokens\n",
        "    attempt_count = 0\n",
        "    retry_wait = BASE_RETRY_DELAY\n",
        "\n",
        "    while attempt_count < RETRY_ATTEMP_MAX:\n",
        "        try:\n",
        "            response = model.generate_content(dialog_history)\n",
        "\n",
        "            if response and hasattr(response, \"usage_metadata\"):\n",
        "                # Extract token usage data from response metadata\n",
        "                usage_data = response.usage_metadata\n",
        "                cumulative_tokens = usage_data.total_token_count\n",
        "                prompt_usage = usage_data.prompt_token_count\n",
        "\n",
        "                print(f\"Token Usage - Total: {cumulative_tokens}, Prompt: {prompt_usage}\")\n",
        "            else:\n",
        "                print(\"API response missing usage metadata\")\n",
        "                cumulative_tokens = 0\n",
        "\n",
        "            return response\n",
        "\n",
        "        except Exception as api_error:\n",
        "            # Handle rate limiting with exponential backoff\n",
        "            if \"rate limit\" in str(api_error).lower() or \"429\" in str(api_error):\n",
        "                print(f\"Rate limited. Retrying in {retry_wait}s (Attempt {attempt_count+1}/{RETRY_ATTEMP_MAX})\")\n",
        "                time.sleep(retry_wait)\n",
        "                attempt_count += 1\n",
        "                retry_wait *= 1.5  # Increase wait time by 50% each retry\n",
        "            else:\n",
        "                print(f\"API communication failed: {api_error}\")\n",
        "                return None\n",
        "\n",
        "    print(\"Maximum retry attempts exhausted. Service unavailable.\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLn0fBrr0IVZ"
      },
      "source": [
        "**Conversation Processing with Context Management and Token Constraints**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ec7L6xzE0EPU"
      },
      "outputs": [],
      "source": [
        "def process_message(user_input):\n",
        "    \"\"\"Manages conversation flow and context window constraints.\"\"\"\n",
        "    global conversation_log\n",
        "    conversation_log.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Context window management loop\n",
        "    while True:\n",
        "        current_dialog = \"\\n\".join(conversation_log)\n",
        "        token_usage = tokenCount(current_dialog)\n",
        "\n",
        "        # Remove older messages when exceeding context limits\n",
        "        if token_usage > CONTEXT_WINDOW_MAX:\n",
        "            # Maintain conversation pairs by removing oldest interactions\n",
        "            removable_items = 2 if len(conversation_log) >= 2 else 1\n",
        "            conversation_log = conversation_log[removable_items:]\n",
        "        else:\n",
        "            # Display warning when approaching context limit\n",
        "            if token_usage >= TOKEN_SAFETY_MARGIN:\n",
        "                print(f\"⚠️ Context window at {token_usage}/{CONTEXT_WINDOW_MAX} tokens\")\n",
        "            break\n",
        "\n",
        "    bot_response = generate_text(current_dialog)\n",
        "\n",
        "    if bot_response:\n",
        "        conversation_log.append(f\"Assistant: {bot_response.text}\")\n",
        "        return bot_response.text\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4z1hX5i0Sy5"
      },
      "source": [
        "**Context Window Enforcement Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBoiDsDl0aCK"
      },
      "outputs": [],
      "source": [
        "def test_context_window():\n",
        "    \"\"\"Validates context window limit enforcement mechanism.\"\"\"\n",
        "    global conversation_log\n",
        "    conversation_log = []\n",
        "    sample_message = (\"Repeating context test pattern \" * 30000).strip()\n",
        "\n",
        "    print(\"\\n=== Initiating Context Window Test ===\")\n",
        "\n",
        "    try:\n",
        "        threshold_80 = int(CONTEXT_WINDOW_MAX * 0.8)  # 80% threshold\n",
        "        warned_80 = False  # Track if 80% warning was issued\n",
        "\n",
        "        while tokenCount(\"\\n\".join(conversation_log)) < CONTEXT_WINDOW_MAX:\n",
        "            # Simulate conversation history growth\n",
        "            conversation_log.extend([\n",
        "                f\"User: {sample_message}\",\n",
        "                \"Assistant: Test response\"\n",
        "            ])\n",
        "            current_tokens = tokenCount(\"\\n\".join(conversation_log))\n",
        "            print(f\"Current token count: {current_tokens}\")\n",
        "\n",
        "            # Warn when crossing 80% threshold (once)\n",
        "            if not warned_80 and current_tokens >= threshold_80:\n",
        "                print(\"-- Warning: Approached 80% of context window\")\n",
        "                warned_80 = True\n",
        "\n",
        "            # Existing safety margin check\n",
        "            if current_tokens >= TOKEN_SAFETY_MARGIN:\n",
        "                print(\"-- Safety threshold notification verified\")\n",
        "\n",
        "        print(\"✔️ Context limit enforcement operational\")\n",
        "\n",
        "    except Exception as test_error:\n",
        "        print(f\"Context test failure: {test_error}\")\n",
        "\n",
        "    print(\"=== Context Test Complete ===\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD0Q0BQW0hSv"
      },
      "source": [
        "**API Rate Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Lp6Sp1w0su3"
      },
      "outputs": [],
      "source": [
        "def test_rate_limit():\n",
        "    \"\"\"Evaluates API rate limit handling capabilities.\"\"\"\n",
        "    global conversation_log, cumulative_tokens\n",
        "    test_payload = \"Stress test message \" * 100  # High token content\n",
        "\n",
        "    print(\"Initializing rate limit evaluation...\")\n",
        "    print(f\"Baseline token count: {cumulative_tokens}\")\n",
        "\n",
        "    while cumulative_tokens < CONTEXT_WINDOW_MAX:\n",
        "        api_result = process_message(test_payload)\n",
        "        if not api_result:\n",
        "            print(\"Terminating test due to API failure\")\n",
        "            break\n",
        "\n",
        "        print(\"API request successful, continuing stress test...\")\n",
        "        time.sleep(0.15)  # Brief pause between requests\n",
        "\n",
        "    print(\"Rate limit evaluation cycle completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP4xpp9Y06Xx"
      },
      "source": [
        "**Interactive Chat Method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RPmOrqL0_mB"
      },
      "outputs": [],
      "source": [
        "def chat():\n",
        "    \"\"\"Main interactive chat interface.\"\"\"\n",
        "    while True:\n",
        "        user_message = input(\"You: \")\n",
        "\n",
        "        if user_message.lower() in ['exit', 'quit']:\n",
        "            print(\"Closing chat session...\")\n",
        "            break\n",
        "\n",
        "        response = process_message(user_message)\n",
        "\n",
        "        if response:\n",
        "            print(f\"Assistant: {response}\")\n",
        "        else:\n",
        "            print(\"Response generation failed. Please retry.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYEXpBGG1D-Y"
      },
      "source": [
        "**Input Text Processing with Token Limit Enforcement**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yx4IHpB7zRm8"
      },
      "outputs": [],
      "source": [
        "def text(input_text):\n",
        "    \"\"\"Processes input text after multiplying its length by 20,000, with token limit enforcement.\"\"\"\n",
        "    expanded_text = input_text * 400000  # Multiply input text length\n",
        "    token_usage = tokenCount(expanded_text)\n",
        "\n",
        "    if token_usage >= CONTEXT_WINDOW_MAX:\n",
        "        return \"❌ Input exceeds maximum token limit. Please shorten your text.\"\n",
        "\n",
        "    if token_usage >= TOKEN_SAFETY_MARGIN:\n",
        "        return f\"⚠️ Warning: Your input is {token_usage}/{CONTEXT_WINDOW_MAX} tokens. Consider shortening it.\"\n",
        "\n",
        "    return process_message(expanded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2sagJFw1mQJ"
      },
      "source": [
        "**Test Chat Mode**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "Jak6LP1semdC",
        "outputId": "113f0a28-d6f1-4e51-aee3-14b3c5cc7897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You: hello\n",
            "Token Usage - Total: 14, Prompt: 3\n",
            "Assistant: Hello there! How can I help you today?\n",
            "\n",
            "You: how are you\n",
            "Token Usage - Total: 42, Prompt: 22\n",
            "Assistant: Assistant: I'm doing well, thank you for asking!  How are you today?\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e57ee30c65ef>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mchat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-7b4a4783018c>\u001b[0m in \u001b[0;36mchat\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Main interactive chat interface.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0muser_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muser_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'exit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "chat()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZMjoyy-2TIc"
      },
      "source": [
        "**Testing Rate Limit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "xBD-4gO8n77Y",
        "outputId": "97103ada-2e62-44de-dac1-5329dbbcca48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing rate limit evaluation...\n",
            "Baseline token count: 42\n",
            "Token Usage - Total: 424, Prompt: 348\n",
            "API request successful, continuing stress test...\n",
            "Token Usage - Total: 794, Prompt: 730\n",
            "API request successful, continuing stress test...\n",
            "Token Usage - Total: 1105, Prompt: 1100\n",
            "API request successful, continuing stress test...\n",
            "Token Usage - Total: 1418, Prompt: 1411\n",
            "API request successful, continuing stress test...\n",
            "Token Usage - Total: 1731, Prompt: 1724\n",
            "API request successful, continuing stress test...\n",
            "Token Usage - Total: 2044, Prompt: 2037\n",
            "API request successful, continuing stress test...\n",
            "Token Usage - Total: 2357, Prompt: 2350\n",
            "API request successful, continuing stress test...\n",
            "Token Usage - Total: 2670, Prompt: 2663\n",
            "API request successful, continuing stress test...\n",
            "Token Usage - Total: 2983, Prompt: 2976\n",
            "API request successful, continuing stress test...\n",
            "Token Usage - Total: 3296, Prompt: 3289\n",
            "API request successful, continuing stress test...\n",
            "Token Usage - Total: 3609, Prompt: 3602\n",
            "API request successful, continuing stress test...\n",
            "Token Usage - Total: 3922, Prompt: 3915\n",
            "API request successful, continuing stress test...\n",
            "Token Usage - Total: 4235, Prompt: 4228\n",
            "API request successful, continuing stress test...\n",
            "Token Usage - Total: 4548, Prompt: 4541\n",
            "API request successful, continuing stress test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 507.36ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limited. Retrying in 2s (Attempt 1/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 508.74ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limited. Retrying in 3.0s (Attempt 2/3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 509.02ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rate limited. Retrying in 4.5s (Attempt 3/3)\n",
            "Maximum retry attempts exhausted. Service unavailable.\n",
            "Terminating test due to API failure\n",
            "Rate limit evaluation cycle completed\n"
          ]
        }
      ],
      "source": [
        "test_rate_limit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldsYVTvX2e1L"
      },
      "source": [
        "**Testing Context Window**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "Y1Uy5jqnn85q",
        "outputId": "86fa31f1-dfea-4070-b70a-1f44e96e5d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Initiating Context Window Test ===\n",
            "Token counting error: Invalid input: 'content' argument must not be empty. Please provide a non-empty value.\n",
            "Current token count: 150007\n",
            "Current token count: 300015\n",
            "Current token count: 450023\n",
            "Current token count: 600031\n",
            "Current token count: 750039\n",
            "Current token count: 900047\n",
            "-- Warning: Approached 80% of context window\n",
            "-- Safety threshold notification verified\n",
            "Current token count: 1050055\n",
            "-- Safety threshold notification verified\n",
            "✔️ Context limit enforcement operational\n",
            "=== Context Test Complete ===\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_context_window()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiTYEju82lIi"
      },
      "source": [
        "**Testing One Input for Token Limit Enforcement**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Au77iYnq0rxD",
        "outputId": "ea86a5cc-a5d5-406a-bb1d-c794f1759fab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'⚠️ Warning: Your input is 800000/1000000 tokens. Consider shortening it.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "text(\"naber\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}